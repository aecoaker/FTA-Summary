{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDXkqQOBWhtn3cA2CRYoqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aecoaker/FTA-Summary/blob/master/Exploring_BART_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring BART Models to find best Pre-Trained Option"
      ],
      "metadata": {
        "id": "QQ5omMGUjzeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of work prediction"
      ],
      "metadata": {
        "id": "HyfleaffDXO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
        "import statistics\n",
        "# load a pre-trained model and tokenizer 'bart-large-cnn'\n",
        "tokeniser = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
      ],
      "metadata": {
        "id": "Btv0UJ7Lj0tE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"There is nothing quite like a sunny day to remind someone of their own mortality.\""
      ],
      "metadata": {
        "id": "XPUeaTOuoRqM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use bart for summary of the sentence to check it all works\n",
        "inputs = tokeniser.batch_encode_plus([text],return_tensors='pt')\n",
        "summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
        "bart_summaries = tokeniser.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(bart_summaries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgRY3Rt6pO5k",
        "outputId": "2f25c673-9a9d-4f01-8aa2-1af3faa5c220"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (142) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is nothing quite like a sunny day to remind someone of their own mortality. There is also nothing like a sun-soaked beach to remind you that you are not immortal. There are no guarantees in life, but there are some things that can be learned from the sun.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create text with a masked word\n",
        "text = \"There is nothing quite like a sunny <mask> to remind someone of their own mortality.\""
      ],
      "metadata": {
        "id": "37zGGt-pRZv-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now use BART to predict what the word is\n",
        "input_ids = tokeniser([text], return_tensors=\"pt\")[\"input_ids\"]\n",
        "logits = model(input_ids).logits\n",
        "masked_index = (input_ids[0] == tokeniser.mask_token_id).nonzero().item()\n",
        "probs = logits[0, masked_index].softmax(dim=0)\n",
        "values, predictions = probs.topk(5) #only get top 5 predictions\n",
        "tokeniser.decode(predictions).split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDZ6OSt9-P_5",
        "outputId": "cbc8be5b-3b94-4af5-bf65-d6216ce95e11"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['day,', 'morning', 'moment', 'afternoon']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing this into a function that generates a metric"
      ],
      "metadata": {
        "id": "OD9iSzaCDqqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(text, model = 'facebook/bart-base'):\n",
        "  #read in chosen model and set up empty lists to use\n",
        "  tokeniser = BartTokenizer.from_pretrained(model)\n",
        "  model = BartForConditionalGeneration.from_pretrained(model)\n",
        "  prod__pp_t = 1\n",
        "  test = []\n",
        "  #tokenise text to get its length\n",
        "  input_ids = tokeniser([text], return_tensors=\"pt\")[\"input_ids\"]\n",
        "  n = len(input_ids[0])\n",
        "  #iterate through tokens\n",
        "  for i in range(1, n):\n",
        "    #get full set of inputs, replace token with '<mask>'\n",
        "    input_ids = tokeniser([text], return_tensors=\"pt\")[\"input_ids\"]\n",
        "    true_token = int(input_ids[0][i])\n",
        "    input_ids[0][i] = 50264\n",
        "    #use BART to predict what this token is\n",
        "    logits = model(input_ids).logits\n",
        "    masked_index = (input_ids[0] == tokeniser.mask_token_id).nonzero().item()\n",
        "    probs = logits[0, masked_index].softmax(dim=0)\n",
        "    values, predictions = probs.topk(1000)\n",
        "    #get the probability of the true token within this prediction\n",
        "    try:\n",
        "      true_token_index = predictions.tolist().index(true_token)\n",
        "      true_token_prob = values[true_token_index].detach().numpy().item()\n",
        "    #deal with words that aren't in the top 1000 predictions by assigning them a very small probability\n",
        "    except:\n",
        "      true_token_prob = 0.00000000001\n",
        "    #calculate the reciprocals of the probabilities and multiple together\n",
        "    test.append(true_token_prob)\n",
        "    pp_t = 1/true_token_prob\n",
        "    prod__pp_t *= pp_t\n",
        "  #calculate the perplexity by normalising this, also (for comparison) show avg probabilities\n",
        "  perplexity = prod__pp_t ** (1/n)\n",
        "  print(perplexity)\n",
        "  print(statistics.mean(test))"
      ],
      "metadata": {
        "id": "Yv_YvIHVDuGb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Amy penned: 'Hey all, I've got some news which isn't easy to share. I've recently been diagnosed with breast cancer but I'm determined to get back on that dance floor before you know it. Welsh love Amy.'  Amy has battled gut condition Crohn's Disease since she was a child and admitted she has already been through 'quite a lot' in her life with her health struggles.\"\n",
        "perplexity(text)"
      ],
      "metadata": {
        "id": "1yStUTHYVz-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Apple cancer rude curtain gaming\"\n",
        "perplexity(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTOSD6nvoGzM",
        "outputId": "ff8d6015-881f-4295-f353-a91abe308c63"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2682695795.2797227\n",
            "[1e-11, 1e-11, 1e-11, 1e-11, 1e-11, 1e-11]\n",
            "1e-11\n"
          ]
        }
      ]
    }
  ]
}