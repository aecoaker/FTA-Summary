{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aecoaker/FTA-Summary/blob/master/Exploring_BART_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ5omMGUjzeP"
      },
      "source": [
        "# Exploring BART Models to find best Pre-Trained Option"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyfleaffDXO6"
      },
      "source": [
        "## Example of work prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Btv0UJ7Lj0tE"
      },
      "outputs": [],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
        "import statistics\n",
        "import time\n",
        "import random\n",
        "# load a pre-trained model and tokenizer 'bart-large-cnn'\n",
        "tokeniser = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPUeaTOuoRqM"
      },
      "outputs": [],
      "source": [
        "text = \"There is nothing quite like a sunny day to remind someone of their own mortality.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgRY3Rt6pO5k",
        "outputId": "2f25c673-9a9d-4f01-8aa2-1af3faa5c220"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (142) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There is nothing quite like a sunny day to remind someone of their own mortality. There is also nothing like a sun-soaked beach to remind you that you are not immortal. There are no guarantees in life, but there are some things that can be learned from the sun.\n"
          ]
        }
      ],
      "source": [
        "#use bart for summary of the sentence to check it all works\n",
        "inputs = tokeniser.batch_encode_plus([text],return_tensors='pt')\n",
        "summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
        "bart_summaries = tokeniser.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(bart_summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37zGGt-pRZv-"
      },
      "outputs": [],
      "source": [
        "#create text with a masked word\n",
        "text = \"There is nothing quite like a sunny <mask> to remind someone of their own mortality.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDZ6OSt9-P_5",
        "outputId": "2cd6ef7b-127a-47f5-a231-a94dce25fc8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['day,', 'morning', 'moment', 'afternoon']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#now use BART to predict what the word is\n",
        "input_ids = tokeniser([text], return_tensors=\"pt\")[\"input_ids\"]\n",
        "logits = model(input_ids).logits\n",
        "masked_index = (input_ids[0] == tokeniser.mask_token_id).nonzero().item()\n",
        "probs = logits[0, masked_index].softmax(dim=0)\n",
        "values, predictions = probs.topk(5) #only get top 5 predictions\n",
        "tokeniser.decode(predictions).split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NiEe5flkE21",
        "outputId": "546fe79b-f108-4c15-9f1d-a125a9975856"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[    0,   970,    16,  1085,  1341,   101,    10,  5419, 50264,     7,\n",
              "          8736,   951,     9,    49,   308, 15812,     4,     2]])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb-lf1xQc1Oz",
        "outputId": "53fff82f-daa4-4a24-a38b-37911b9458e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[970, 16, 1085, 1341, 101, 10, 5419, 50264, 7]]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[input_ids[0][1:10].detach().numpy().tolist()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD9iSzaCDqqx"
      },
      "source": [
        "## Writing this into a function that generates a metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Yv_YvIHVDuGb"
      },
      "outputs": [],
      "source": [
        "def perplexity(text, model = 'facebook/bart-base'):\n",
        "  #read in chosen model and set up empty lists to use\n",
        "  tokeniser = BartTokenizer.from_pretrained(model)\n",
        "  model = BartForConditionalGeneration.from_pretrained(model)\n",
        "  prod__pp_t = 1\n",
        "  probs = []\n",
        "  #tokenise text to get its length\n",
        "  input_ids = tokeniser([text], return_tensors=\"pt\")[\"input_ids\"]\n",
        "  n = len(input_ids[0])\n",
        "  #iterate through tokens\n",
        "  for i in range(1, n):\n",
        "    #get full set of inputs, find real value then replace token with '<mask>'\n",
        "    input_ids = tokeniser([text], return_tensors=\"pt\")[\"input_ids\"]\n",
        "    true_token = int(input_ids[0][i])\n",
        "    input_ids[0][i] = 50264\n",
        "    #find sliding window of tokens to use as context\n",
        "    if n < 1024:\n",
        "      window_start = 0\n",
        "      window_end = n\n",
        "    else:\n",
        "      window_start = i - 512\n",
        "      window_end = i + 512\n",
        "    if window_start < 0:\n",
        "      window_start = 0\n",
        "      window_end += -(i - 512)\n",
        "    if window_end > n:\n",
        "      window_end = n\n",
        "      window_start += -(n - i - 512)\n",
        "    #subset the input_ids to only look at the sliding window\n",
        "    input_ids = input_ids[:, window_start:window_end]\n",
        "    #use BART to predict what this token is given the context in the window\n",
        "    logits = model(input_ids).logits\n",
        "    masked_index = (input_ids[0] == tokeniser.mask_token_id).nonzero().item()\n",
        "    probs = logits[0, masked_index].softmax(dim=0)\n",
        "    values, predictions = probs.topk(1000)\n",
        "    #get the probability of the true token within this prediction\n",
        "    try:\n",
        "      true_token_index = predictions.tolist().index(true_token)\n",
        "      true_token_prob = values[true_token_index].detach().numpy().item()\n",
        "    #deal with words that aren't in the top 1000 predictions by assigning them a very small probability\n",
        "    except:\n",
        "      true_token_prob = 0.00000000001\n",
        "    #calculate the reciprocals of the probabilities and multiple together\n",
        "    probs.append(true_token_prob)\n",
        "    pp_t = 1/true_token_prob\n",
        "    prod__pp_t *= pp_t\n",
        "  #calculate the perplexity by normalising this, also (for comparison) show avg probabilities\n",
        "  perplexity = prod__pp_t ** (1/n)\n",
        "  print(perplexity)\n",
        "  print(statistics.mean(probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yStUTHYVz-R",
        "outputId": "4de91ecd-ccac-412b-f6dd-eac1d0431981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15.062050290979776\n",
            "0.3582748056628363\n"
          ]
        }
      ],
      "source": [
        "text = \"After receipt by a Party's investigating authority of a properly documented application for an anti-dumping investigation or a countervailing duty investigation with respect to imports from the other Party and before initiating an investigation, the importing Party shall provide written notification to the other Party of its receipt of the application. \"\n",
        "start = time.perf_counter()\n",
        "perplexity(text)\n",
        "end = time.perf_counter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTOSD6nvoGzM",
        "outputId": "9996f195-5fa8-4fa8-a4f6-61b1184bb433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text had 51 words and it took 26.51806341400004 seconds to run\n"
          ]
        }
      ],
      "source": [
        "print('The text had ' + str(len(text.split())) + ' words and this took ' + str(end - start) + ' seconds to run')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#re-writing function to sample a window from the text, and use that instead of the full text in order to allow us to measure for FTAs\n",
        "def perplexity(text, model = 'facebook/bart-base'):\n",
        "  #read in chosen model and set up empty lists to use\n",
        "  tokeniser = BartTokenizer.from_pretrained(model)\n",
        "  model = BartForConditionalGeneration.from_pretrained(model)\n",
        "  prod__pp_t = 1\n",
        "  test = []\n",
        "  #tokenise text to get its length\n",
        "  input_ids = tokeniser([text], return_tensors=\"pt\")[\"input_ids\"]\n",
        "  n = len(input_ids[0])\n",
        "  #iterate through tokens\n",
        "  for i in range(1, n):\n",
        "    #get full set of inputs, find real value then replace token with '<mask>'\n",
        "    input_ids = tokeniser([text], return_tensors=\"pt\")[\"input_ids\"]\n",
        "    true_token = int(input_ids[0][i])\n",
        "    input_ids[0][i] = 50264\n",
        "    #find sliding window of tokens to use as context\n",
        "    if n < 1024:\n",
        "      window_start = 0\n",
        "      window_end = n\n",
        "    else:\n",
        "      window_start = i - 512\n",
        "      window_end = i + 512\n",
        "    if window_start < 0:\n",
        "      window_start = 0\n",
        "      window_end += -(i - 512)\n",
        "    if window_end > n:\n",
        "      window_end = n\n",
        "      window_start += -(n - i - 512)\n",
        "    #subset the input_ids to only look at the sliding window\n",
        "    input_ids = input_ids[:, window_start:window_end]\n",
        "    #use BART to predict what this token is given the context in the window\n",
        "    logits = model(input_ids).logits\n",
        "    masked_index = (input_ids[0] == tokeniser.mask_token_id).nonzero().item()\n",
        "    probs = logits[0, masked_index].softmax(dim=0)\n",
        "    values, predictions = probs.topk(1000)\n",
        "    #get the probability of the true token within this prediction\n",
        "    try:\n",
        "      true_token_index = predictions.tolist().index(true_token)\n",
        "      true_token_prob = values[true_token_index].detach().numpy().item()\n",
        "    #deal with words that aren't in the top 1000 predictions by assigning them a very small probability\n",
        "    except:\n",
        "      true_token_prob = 0.00000000001\n",
        "    #calculate the reciprocals of the probabilities and multiple together\n",
        "    test.append(true_token_prob)\n",
        "    pp_t = 1/true_token_prob\n",
        "    prod__pp_t *= pp_t\n",
        "  #calculate the perplexity by normalising this, also (for comparison) show avg probabilities\n",
        "  perplexity = prod__pp_t ** (1/n)\n",
        "  print(perplexity)\n",
        "  print(statistics.mean(test))"
      ],
      "metadata": {
        "id": "HeYu0t-x8XAP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPkWy8nAcivl86v2QdxzRhm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}