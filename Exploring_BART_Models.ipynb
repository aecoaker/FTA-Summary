{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUrMUzaTxmR4UAIMrkJ3UP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aecoaker/FTA-Summary/blob/master/Exploring_BART_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring BART Models to find best Pre-Trained Option"
      ],
      "metadata": {
        "id": "QQ5omMGUjzeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of work prediction"
      ],
      "metadata": {
        "id": "HyfleaffDXO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
        "# load a pre-trained model and tokenizer 'bart-large-cnn'\n",
        "tokeniser = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
      ],
      "metadata": {
        "id": "Btv0UJ7Lj0tE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"There is nothing quite like a sunny day to remind someone of their own mortality.\""
      ],
      "metadata": {
        "id": "XPUeaTOuoRqM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use bart for summary of the sentence to check it all works\n",
        "inputs = tokeniser.batch_encode_plus([text],return_tensors='pt')\n",
        "summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
        "bart_summaries = tokeniser.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(bart_summaries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgRY3Rt6pO5k",
        "outputId": "2f25c673-9a9d-4f01-8aa2-1af3faa5c220"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (142) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is nothing quite like a sunny day to remind someone of their own mortality. There is also nothing like a sun-soaked beach to remind you that you are not immortal. There are no guarantees in life, but there are some things that can be learned from the sun.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create text with a masked word\n",
        "text = \"There is nothing quite like a sunny <mask> to remind someone of their own mortality.\""
      ],
      "metadata": {
        "id": "37zGGt-pRZv-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now use BART to predict what the word is\n",
        "input_ids = tokeniser([text], return_tensors=\"pt\")[\"input_ids\"]\n",
        "logits = model(input_ids).logits\n",
        "masked_index = (input_ids[0] == tokeniser.mask_token_id).nonzero().item()\n",
        "probs = logits[0, masked_index].softmax(dim=0)\n",
        "values, predictions = probs.topk(5) #only get top 5 predictions\n",
        "tokeniser.decode(predictions).split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDZ6OSt9-P_5",
        "outputId": "cbc8be5b-3b94-4af5-bf65-d6216ce95e11"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['day,', 'morning', 'moment', 'afternoon']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing this into a function that generates a metric"
      ],
      "metadata": {
        "id": "OD9iSzaCDqqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_pred_good(text, model = 'facebook/bart-base'):\n",
        "  #read in chosen model\n",
        "  tokeniser = BartTokenizer.from_pretrained(model)\n",
        "  model = BartForConditionalGeneration.from_pretrained(model)\n",
        "  #tokenise text and sample 10% of tokens from it\n",
        "  input_ids = tokeniser([text], return_tensors=\"pt\")[\"input_ids\"]\n",
        "  n = len(input_ids[0])\n",
        "  n_masks = int(n/10)\n",
        "  masks_sample = random.sample(range(1, n), n_masks) #avoid the first and last tokens which are static\n",
        "  #iterate through the sampled tokens\n",
        "  for i in range(n_masks):\n",
        "    print('loop ' + str(i))\n",
        "    #replace sampled token with '<mask>'\n",
        "    input_ids = tokeniser([text], return_tensors=\"pt\")[\"input_ids\"]\n",
        "    true_token = int(input_ids[0][masks_sample[i]])\n",
        "    input_ids[0][masks_sample[i]] = 50264\n",
        "    #use BART to predict what this token is\n",
        "    logits = model(input_ids).logits\n",
        "    masked_index = (input_ids[0] == tokeniser.mask_token_id).nonzero().item()\n",
        "    probs = logits[0, masked_index].softmax(dim=0)\n",
        "    values, predictions = probs.topk(500)\n",
        "    #get the probability of the true token within this prediction\n",
        "    try:\n",
        "      true_token_index = predictions.tolist().index(true_token)\n",
        "      true_token_prob = values[true_token_index]\n",
        "    except:\n",
        "      true_token_prob = 0\n",
        "    print(true_token_prob)"
      ],
      "metadata": {
        "id": "Yv_YvIHVDuGb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Amy penned: beep beep boop boop beep beep boop boop beep beep boop boop 'Hey all, I've got some news which isn't easy to share. I've recently been diagnosed with breast cancer but I'm determined to get back on that dance floor before you know it. Welsh love Amy.'  Amy has battled gut condition Crohn's Disease since she was a child and admitted she has already been through 'quite a lot' in her life with her health struggles.\"\n",
        "is_pred_good(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omQ8VE9FELxB",
        "outputId": "62d98bfb-85b3-4d92-ec4f-6ee9c4b2828d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loop 0\n",
            "tensor(0.9969, grad_fn=<SelectBackward0>)\n",
            "loop 1\n",
            "0\n",
            "loop 2\n",
            "tensor(0.0112, grad_fn=<SelectBackward0>)\n",
            "loop 3\n",
            "tensor(0.7863, grad_fn=<SelectBackward0>)\n",
            "loop 4\n",
            "tensor(0.3439, grad_fn=<SelectBackward0>)\n",
            "loop 5\n",
            "tensor(0.5515, grad_fn=<SelectBackward0>)\n",
            "loop 6\n",
            "tensor(0.0357, grad_fn=<SelectBackward0>)\n",
            "loop 7\n",
            "tensor(0.9620, grad_fn=<SelectBackward0>)\n",
            "loop 8\n",
            "tensor(0.2836, grad_fn=<SelectBackward0>)\n",
            "loop 9\n",
            "tensor(0.0419, grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    }
  ]
}